{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from duckduckgo_search import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "DEFAULT_TIMEOUT = 10\n",
    "DEFAULT_DELAY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url: str, headers: Dict = None, timeout: int = DEFAULT_TIMEOUT) -> requests.Response:\n",
    "    \"\"\"Make HTTP request with error handling.\"\"\"\n",
    "    try:\n",
    "        headers = headers or DEFAULT_HEADERS\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error making request to {url}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def safe_soup_parse(html_content: str) -> BeautifulSoup:\n",
    "    \"\"\"Safely parse HTML content with BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        return BeautifulSoup(html_content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing HTML: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_duckduckgo_result(\n",
    "    query: str,\n",
    "    num_results: int = 1,\n",
    "    include_html: bool = True,\n",
    "    region: str = 'wt-wt',\n",
    "    safesearch: str = 'moderate',\n",
    "    timelimit: str | None = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced DuckDuckGo search function with error handling and logging.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query string\n",
    "        num_results (int): Number of results to retrieve (default: 1)\n",
    "        include_html (bool): Whether to include HTML content in result (default: True)\n",
    "        region (str): Region code for search results (default: worldwide)\n",
    "        safesearch (str): SafeSearch setting ('on', 'moderate', 'off')\n",
    "        timelimit (str): Time limit for results ('d' for day, 'w' for week, 'm' for month, 'y' for year)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Search result information including URL, description, and metadata\n",
    "    \"\"\"\n",
    "    logger.info(f\"Searching DuckDuckGo for: {query}\")\n",
    "    try:\n",
    "        # Initialize DuckDuckGo search\n",
    "        ddgs = DDGS()\n",
    "        \n",
    "        # Get search results\n",
    "        search_results = list(ddgs.text(\n",
    "            keywords=query,\n",
    "            region=region,\n",
    "            safesearch=safesearch,\n",
    "            timelimit=timelimit,\n",
    "            max_results=num_results\n",
    "        ))\n",
    "        \n",
    "        if not search_results:\n",
    "            logger.warning(\"No results found for query\")\n",
    "            return {\"error\": \"No results found\", \"success\": False}\n",
    "            \n",
    "        first_result = search_results[0]\n",
    "        \n",
    "        # Build result dictionary with available fields\n",
    "        result = {\n",
    "            \"url\": first_result.get('link') or first_result.get('href'),\n",
    "            \"title\": first_result.get('title', 'No title found'),\n",
    "            \"description\": first_result.get('body', ''),\n",
    "            \"success\": True,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if include_html:\n",
    "            try:\n",
    "                # Use the DDGS text function with html backend to get HTML content\n",
    "                html_results = list(ddgs.text(\n",
    "                    keywords=query,\n",
    "                    region=region,\n",
    "                    safesearch=safesearch,\n",
    "                    timelimit=timelimit,\n",
    "                    backend='html',\n",
    "                    max_results=1\n",
    "                ))\n",
    "                if html_results:\n",
    "                    result[\"html_content\"] = html_results[0].get('html', '')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not retrieve HTML content: {str(e)}\")\n",
    "                result[\"html_content\"] = None\n",
    "        \n",
    "        logger.info(f\"Successfully retrieved result for: {result['url']}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_first_duckduckgo_result: {str(e)}\")\n",
    "        return {\"error\": str(e), \"success\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_content(url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced HTML cleaning function with better text processing.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cleaning HTML content from: {url}\")\n",
    "    try:\n",
    "        # Fetch content\n",
    "        response = make_request(url)\n",
    "        soup = safe_soup_parse(response.text)\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup.select('script, style, img, link, iframe, header, footer, nav, [class*=\"ads\"], .advertisement'):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Process links\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.replace_with(a_tag.text)\n",
    "        \n",
    "        # Extract and clean text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Enhanced text cleaning\n",
    "        lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 1:  # Skip single-character lines\n",
    "                lines.append(line)\n",
    "        \n",
    "        text = ' '.join(lines)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\-.,;:!?«»àâäéèêëîïôöùûüÿçÀÂÄÉÈÊËÎÏÔÖÙÛÜŸÇ]', '', text)\n",
    "        \n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"cleaned_content\": text.strip(),\n",
    "            \"original_length\": len(response.text),\n",
    "            \"cleaned_length\": len(text.strip()),\n",
    "            \"success\": True,\n",
    "            \"status_code\": response.status_code,\n",
    "            \"content_type\": response.headers.get('content-type', ''),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Successfully cleaned content from: {url}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in clean_html_content: {str(e)}\")\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"cleaned_content\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"communes prefecture \\\"Agadir Ida-Outanane\\\" wikipedia\"\n",
    "result = get_first_duckduckgo_result(query, include_html=False)\n",
    "clean_result = clean_html_content(result['url'])\n",
    "print(clean_result['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "# Load the region divisions data\n",
    "with open('../data/region-divisions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list to store processed divisions\n",
    "results = []\n",
    "\n",
    "# Process each division\n",
    "for division in data['region_divisions']:\n",
    "    # Create query combining type and French name\n",
    "    query = f\"les communes de {division['type']} de \\\"{division['name']['fr']}\\\" wikipedia\"\n",
    "    \n",
    "    # Get wiki content using your existing function\n",
    "    raw_content = get_first_duckduckgo_result(query, include_html=False)\n",
    "    # retry 8 times if failed\n",
    "    retry = 8\n",
    "    for i in range(retry):\n",
    "        if raw_content['success']:\n",
    "            break\n",
    "        time.sleep(3)\n",
    "        raw_content = get_first_duckduckgo_result(query, include_html=False)\n",
    "    \n",
    "\n",
    "    if raw_content['success']:\n",
    "        clean_content = clean_html_content(raw_content[\"url\"])\n",
    "        content = clean_content[\"cleaned_content\"]\n",
    "    else:\n",
    "        content = \"\"\n",
    "    \n",
    "    # Create result object\n",
    "    result = {\n",
    "        \"id\": division['id'],\n",
    "        \"name\": division['name']['en'],\n",
    "        \"content\": content\n",
    "    }\n",
    "    \n",
    "    # Add to results list\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress (optional)\n",
    "    print(f\"Processed {division['id']}: {division['name']['en']}\")\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\"divisions\": results}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Print completion message\n",
    "print(f\"\\nProcessing complete! Processed {len(results)} divisions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "class CommuneName(BaseModel):\n",
    "    fr: str\n",
    "    en: str\n",
    "    ar: str\n",
    "    es: str\n",
    "\n",
    "class Commune(BaseModel):\n",
    "    name: CommuneName\n",
    "    type: str  # Can be \"urbain\", \"rurale\", or empty string\n",
    "    region_division_id: str\n",
    "    comments: List[str]\n",
    "\n",
    "class ExtractedCommunes(BaseModel):\n",
    "    communes: List[Commune]\n",
    "    division_comments: List[str]\n",
    "\n",
    "class ProcessingResult(BaseModel):\n",
    "    communes: List[Commune]\n",
    "    processing_comments: List[str]\n",
    "    metadata: dict\n",
    "\n",
    "def translate_name(name: str) -> CommuneName:\n",
    "    \"\"\"Translate commune name to multiple languages.\"\"\"\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate this Moroccan commune name: {name}\"\n",
    "            }],\n",
    "            response_format=CommuneName\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error for {name}: {str(e)}\")\n",
    "        return CommuneName(fr=name, en=name, ar=name, es=name)\n",
    "\n",
    "def extract_communes_from_wiki(content: dict) -> ExtractedCommunes:\n",
    "    \"\"\"Extract communes from wiki content using GPT-4o.\"\"\"\n",
    "    try:\n",
    "        # Safely access dictionary content with get() method and provide defaults\n",
    "        title = content.get('name', 'Unknown Title')\n",
    "        # Check if content is a dictionary and has 'full_content'\n",
    "        full_content = content.get('content', '')\n",
    "\n",
    "        division_id = content.get('id', 'Unknown ID')\n",
    "        \n",
    "        if not isinstance(full_content, str):\n",
    "            raise ValueError(f\"Invalid content format for division {division_id}. Expected string, got {type(full_content)}\")\n",
    "\n",
    "        # Print debug information\n",
    "        print(f\"Processing content for {division_id}:\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Content length: {len(str(full_content))} characters\")\n",
    "\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Extract and classify Moroccan communes from the provided content.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Analyze this content and extract communes:\n",
    "                    Title: {title}\n",
    "                    Content: {full_content}\n",
    "                    \n",
    "                    Rules:\n",
    "                    - Classify as 'urbain' for cities/municipalities\n",
    "                    - Classify as 'rurale' for villages/rural areas\n",
    "                    - Leave type empty if unclear\n",
    "                    - Add relevant comments for uncertainties\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            response_format=ExtractedCommunes\n",
    "        )\n",
    "        \n",
    "        result = completion.choices[0].message\n",
    "        \n",
    "        if hasattr(result, 'refusal') and result.refusal:\n",
    "            return ExtractedCommunes(\n",
    "                communes=[],\n",
    "                division_comments=[f\"Content processing refused: {result.refusal}\"]\n",
    "            )\n",
    "            \n",
    "        # Process each commune to add translations\n",
    "        processed_communes = []\n",
    "        for commune in result.parsed.communes:\n",
    "            translations = translate_name(commune.name.fr)\n",
    "            \n",
    "            processed_commune = Commune(\n",
    "                name=translations,\n",
    "                type=commune.type,\n",
    "                region_division_id=division_id,\n",
    "                comments=commune.comments\n",
    "            )\n",
    "            processed_communes.append(processed_commune)\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "        return ExtractedCommunes(\n",
    "            communes=processed_communes,\n",
    "            division_comments=result.parsed.division_comments\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {division_id}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        # Print more detailed debug information\n",
    "        print(f\"Content type: {type(content)}\")\n",
    "        print(f\"Content keys: {content.keys() if isinstance(content, dict) else 'Not a dictionary'}\")\n",
    "        return ExtractedCommunes(\n",
    "            communes=[],\n",
    "            division_comments=[error_msg]\n",
    "        )\n",
    "\n",
    "def process_divisions(input_file: str, output_file: str) -> None:\n",
    "    print(\"Loading division results...\")\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            wiki_results = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading input file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    if not isinstance(wiki_results, dict) or 'divisions' not in wiki_results:\n",
    "        print(\"Invalid input file format: missing 'divisions' key or not a dictionary\")\n",
    "        return\n",
    "\n",
    "    all_communes = []\n",
    "    all_comments = []\n",
    "    total_divisions = len(wiki_results['divisions'])\n",
    "\n",
    "    for idx, division in enumerate(wiki_results['divisions'], 1):\n",
    "        print(f\"\\nProcessing division {idx}/{total_divisions}: {division.get('name', 'Unknown')}\")\n",
    "        \n",
    "        try:\n",
    "            if not isinstance(division, dict):\n",
    "                raise ValueError(f\"Division {idx} is not a dictionary\")\n",
    "                \n",
    "            if 'content' not in division or 'id' not in division:\n",
    "                raise ValueError(f\"Division {idx} missing required fields\")\n",
    "\n",
    "            result = extract_communes_from_wiki(division)\n",
    "            \n",
    "            if result.division_comments:\n",
    "                all_comments.extend([\n",
    "                    f\"Division {division['id']} ({division.get('name', 'Unknown')}): {comment}\"\n",
    "                    for comment in result.division_comments\n",
    "                ])\n",
    "            \n",
    "            all_communes.extend(result.communes)\n",
    "            print(f\"Extracted {len(result.communes)} communes\")\n",
    "            time.sleep(1)  # Rate limiting between divisions\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to process division {division.get('id', 'Unknown')}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            all_comments.append(error_msg)\n",
    "            continue\n",
    "\n",
    "    # Create final result\n",
    "    output = ProcessingResult(\n",
    "        communes=all_communes,\n",
    "        processing_comments=all_comments,\n",
    "        metadata={\n",
    "            \"total_communes\": len(all_communes),\n",
    "            \"total_divisions_processed\": total_divisions,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output.model_dump(), f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nExtraction complete!\")\n",
    "    print(f\"Total communes extracted: {len(all_communes)}\")\n",
    "    print(f\"Total comments/notes: {len(all_comments)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_divisions('result.json', 'communes.json')\n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
